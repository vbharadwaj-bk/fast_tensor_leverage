\section{Statistical Leverage Scores}
\textit{Randomized sketching} is a well-studied tool to reduce the
cost of linear least squares problems. Given a problem of the form
$\min_X \norm{AX - B}_F$, we apply a sketching operator to both
$A$ and $B$ and solve the reduced problem $\min_X \norm{SAX - SB}_F$. 
There exist several popular choices for $S$. Here, we choose $S$
as a \textit{sampling matrix} that selects rows according to the
distribution of statistical leverage scores on $A$. The leverage 
score of row $i$ is given by
\begin{equation}
\ell_i = A\br{i, :} (A^T A)^+ A \br{i, :}^T
\label{eq:leverage_definition}
\end{equation}
for $1 \leq i \leq n$. If we assign 
$C = \paren{\sum_{i=1}^n \ell_i}^{-1}$, then the
probability mass of row $i$ according to the distribution induced
by the leverage scores is $\hat \ell_i = C \ell_i$. Leverage 
scores assign a measure of importance to each row of $A$.
The next theorem, which has appeared in several forms over
multiple works, guarantees that the residual of the reduced
problem is close to the residual of the original problem,
provided an appropriate number of rows selected: 

\begin{theorem}[Guarantees for Leverage Score Sampling, Malik et al. 2022] 
Given $A \in \RR^{I \times R}$ and $\varepsilon, \delta \in (0, 1)$, 
let $S \in \RR^{J \times I}$ be a leverage score sampling matrix
for $A$. Further define $\textrm{OPT} = \min_X \norm{AX - B}_F$
and $$\tilde X = \textrm{argmin}_X \norm{SAX - SB}_F.$$ If $J \gtrsim
R \max \paren{\log \paren{R / \delta}, 1 / (\varepsilon \delta)}$, then
with probability at least $1 - \delta$ it holds that

$$\norm{A \tilde X - B}_F \leq (1 + \varepsilon)\textrm{OPT}.$$
\label{thm:lev_score_lowerbounds}
\end{theorem}
For the applications considered in this work, $R$ ranges from 25
to 256. As $\varepsilon$ and $\delta$ tend to 0 with fixed $R$, 
$1 / (\varepsilon \delta)$ dominates $\log(R / \delta)$. Hence, 
we assume that the minimum sample count $J$ to achieve the
guarantees of the theorem is a linear multiple of 
$1 / (\varepsilon \delta)$.

\section{Partition Tree Sampling}
Suppose we want to sample indices from the set $\set{1, ..., n}$ 
according to a probability 
distribution $r = \set{r_1, ..., r_n}$ on its elements. When the probabilities $r_i$ are known 
in advance and are the same for all samples, several efficient methods exist. 
An alias table, for example, is a data structure with $O(n)$
construction time that yields each sample in constant time.
In our case, however, each sample will be drawn from a potentially 
distinct distribution from some large family $\scr F$. 
Rebuilding the alias table for each sample defeats the purpose 
of the data structure. To draw samples efficiently, we will need 
some additional properties for $\scr F$ and a sampler that can
take advantage of these properties. We begin by describing 
the sampler.

\begin{figure}
    \centering

\scalebox{1.0}{
\input{img/partition_tree.tikz}
}
    \caption{A partition tree $T_{8,2}$ and probability distribution $\set{r_1, ..., r_8}$ on 
    $\br{1, ..., 8}$. The subset owned by each node is indicated. 
    The subsets at the leaves are illustrated as contiguous intervals, but this is not a requirement.}
    \label{fig:part_tree}
\end{figure}

Our sampling method relies on a random walk on a 
\textit{partition tree}, illustrated in figure \ref{fig:part_tree}. 
A partition tree $T_{n, F} = \set{v_1, v_2, ...}$ of 
is a full binary tree of 
\textit{minimal depth} where each 
each node is equipped with a non-empty subset $S(v) \subseteq \set{1, ..., n}$ satisfying the following: 

\begin{enumerate}
    \item The subsets owned by leaves form a partition of $\set{1..n}$, i.e. 
    $\bigcup_{i \in \lf{T_{n, F}}} S(v) = \set{1, ... n}$, and $S(v_i) \cap S(v_j) = \varnothing$ for
    all $v_i, v_j \in \lf{T_{n, F}}$.

    \item For $v \in \lf{T_{n, F}}$, $\abs{S(v)} \leq F$: subsets held by leaves have cardinality
    at most $F$.

    \item Let $L(v), R(v)$ denote the left, resp. right children
    of any non-leaf node. For $v \notin \lf{T_{n, F}}$, $S(v) = S(L(v)) \cup S(R(v))$ (or $S(v) = S(L(v))$. The interval held by a non-leaf is the union of intervals held by its children.
\end{enumerate} 

From the construction of the tree, the root node satisfies $S(\rt{T_{n, F}}) = \set{1, .., n}$. 
Now consider the random process detailed in algorithm \ref{alg:partition_sampling}. In addition to 
a partition tree, this random process takes two functions on the nodes of tree: 
$m: T_{n, F} \mapsto \br{0, 1}$ and $q: T_{n, F} \mapsto 
\br{0, 1}^F$. The process traverses the tree from the
root to a leaf according to the following process: at any non-leaf node $c$, we flip a coin 
with bias $m(L(c)) / m(c)$. If heads, we update $c$ by jumping to its left child, 
and otherwise we jump to its right child. When 
$c$ becomes a leaf, we roll a loaded die with face probabilities proportional to the
vector $q(c)$. Finally, we select the entry in $S(c)$ corresponding to the outcome of the die roll
and return it. The next proposition shows that an appropriate choice of functions $m, q$ turns this random process into a sampler for 
any probability distribution $\set{r_1, ..., r_n}$:

\begin{proposition}[Partition Tree Sampling Correctness]
Suppose, for some constant $C$, that 
$$m(v) = C \sum_{i \in S(v)} r_i$$ 
and
$$q(v) = \set{C r_i\ \vert\ i \in S(v)}$$
for a probability distribution $\set{r_1, ..., r_n}$. \footnote{If there
are fewer than $F$ elements in $S(v)$, $q(v)$ should pad its result with 0.} Then the probability that $\textrm{PTSample}(T_{n, F}, m, q)$ visits 
$v \in T_{n, F}$ is $m(v) / C$, and the probability that the
procedure returns index $i$ is $r_i$.
\label{prop:ptsample_correctness}
\end{proposition}

\begin{algorithm}
    \caption{Sampling via Partition Tree Traversal}
    \label{alg:partition_sampling}
    \begin{algorithmic}[1]
    \Procedure{PTSample}{$T_{n,F}$, $m(\cdot)$, $q(\cdot)$}
    \State Let $c := \rt{T_{n,F}}, m_c := m(c)$
    \While {$c \notin \lf{T_{n,F}}$}
        \State $m_L := m(L(c))$
        \State $R_c \sim \textrm{binom} \paren{\set{0, 1}, m_L / m_c}$
        \If{$R_c = 1$}
            \State $c := L(c)$
            \State $m_c := m_L$
        \Else 
            \State $c := R(c)$
            \State $m_c := m_c - m_L$
        \EndIf
    \EndWhile
    \State $R_c \sim \textrm{multinomial} \paren{S(c), q(c)}$ 
    \State \textbf{return} $R_c$  
    \EndProcedure
    \end{algorithmic}
\end{algorithm}

\begin{proof}
The first part of the statement (probability of visiting each node) 
follows by induction on the levels of the binary tree, where 
the level of a node is the number of edges 
along its path to the root. For the base case, observe that PTSample 
always visits the root, and we have

$$m(\textrm{root}(T_{n, F})) / C = \paren{C \sum_{i=1}^n r_i} / C  = C / C = 1,$$
Now assume that the statement holds for all nodes at level $i$ of the tree; then for node
$v$ at level $i + 1$, let $\textrm{root}(T_{n, F}) =u_1 \rightarrow u_2, \rightarrow ... \rightarrow  u_i \rightarrow u_{i+1} = v$ be the 
unique path from the root to $v$. If $v$ is a left child, it is visited if the binomial draw at node $u_i$ is 1. Hence, 
\begin{aligned_eq}
p(v \textrm{ visited}) 
&= p(u_i\textrm{ visited}) p(R_{u_i} = 1) \\
&= p(u_i\textrm{ visited}) \frac{m(L(u_i))}{m(u_i)} & \just{Line 4, algorithm \ref{alg:partition_sampling}} \\
&= \paren{\frac{m(u_i)}{C}} \paren{\frac{m(L(u_i))}{m(u_i)}} & \just{Induction Hypothesis} \\
&= m(L(u_i)) / C  \\
&= m(v) / C
\end{aligned_eq}
Similarly, when $v$ is a right child of $u_i$, we have
\begin{aligned_eq}
p(v \textrm{ visited}) 
&= p(u_i\textrm{ visited}) p(R_{u_i} = 0) \\
&= \paren{\frac{m(u_i)}{C}} \paren{1 - \frac{m(L(u_i))}{m(u_i)}} \\
&=  \paren{\frac{m(u_i)}{C}} \paren{\frac{m(R(u_i))}{m(u_i)}} \\ 
&= m(R(u_i)) / C \\
&= m(v) / C
\end{aligned_eq}
The third step in the sequence above uses the fact $m(v) = m(L(v)) + m(R(v))$, which is easily
proven from the definition of $m$ in the theorem and the fact 
$S(v) = S(L(v)) \cup S(R(v))$. The first part of the proposition
follows by induction. To prove the 
second part of the statement (probability of drawing index $i$), 
suppose $i \in S(v)$ for some leaf node $v$.
Index $i$ is selected iff the PTSample procedure reaches $v$ and then selects $i$ from the 
multinomial draw. The probability of the multinomial draw selecting $i \in S$ is proportional 
to $C r_i$; normalizing gives 

$$p(R_v = i) = \frac{C q_i}{\sum_{i \in S} C r_i} = \frac{C r_i}{m(v)}.$$
Therefore, we have

\begin{aligned_eq}
p(\textrm{index $i$ selected}) 
&= p(v\textrm{ visited}) p\paren{R_v = i} \\
&= \paren{\frac{m(v)}{C}} p\paren{R_v = i} \\
&= \paren{\frac{m(v)}{C}} \paren{\frac{C r_i}{m(v)}} \\ 
&= r_i
\end{aligned_eq}
\end{proof}
The bound on the sizes of leaf subsets and the fact that our binary tree is full 
imply that the depth of $T_{n, F}$ is $\ceil{\log \ceil{n/F}}$. Suppose now that we have 
procedures to compute $m(v)$ in time $\tau_1$ for all $v \in T_{n, F}$ and 
$q(v)$ in time $\tau_2(F)$ for $v \in \lf{T_{n, F}}$. Then the runtime 
of PTSample is 
\begin{equation}
\tau_1 \ceil{\log \ceil{n/F}} + \tau_2(F)
\label{eq:ptsample_runtime}
\end{equation}
As \ref{eq:ptsample_runtime} shows, we require efficient functions
$m(v)$ and $q(v)$ to gain any benefit from our sampling
procedure. When the distribution $r$ changes from sample to sample, we 
face the same challenge as the alias table: for each draw, 
we cannot spend $O(n)$ time to recompute $m(v)$ and $q(v)$ for
all nodes (resp. leaves) in the partition tree. In the next section,
we examine a specific family of distributions $\scr F$ that, given a
preprocessing step with time linear in $n$, allows $m(v)$ and
$q(v)$ to be computed in time independent of $n$ for any 
$r \in \scr F$.

\section{Leverage-Score Sampling from a KRP}
\label{sec:krp_leverage_defs}
Given matrices $U_1, ..., U_{N-1}$, $N \geq 2$ and $U_j \in \RR^{\abs{I_j} \times R}$ for all $j$, consider the Khatri-Rao product 
$$A = U_1 \odot ... \odot U_{N-1}.$$
We wish to sample rows from $A$
according to the leverage score distribution on its rows. Let us 
index each row of $A$ by a tuple $(i_1, ..., i_{N-1}) \in
I_1 \times ... \times I_{N-1}$.
Equation \ref{eq:leverage_definition} gives
\begin{equation}
\ell_{i_1, ..., i_{N-1}} = 
A \br{(i_1, ..., i_{N-1}), :} (A^T 
A)^+ A \br{(i_1, ..., i_{N-1}), :}^T
\label{eq:krp_leverage}
\end{equation}
The height of $A$ is $\prod_{k=1}^{N-1} \abs{I_k}$, making
it impractical to explicitly compute and store all leverage scores.
Instead, we take each sample by drawing an index from each of 
$I_1, ..., I_{N-1}$ in sequence. For one row sample,
let $\hat i_1, ..., \hat i_{N-1}$ be random variables for 
the draw from each index set according to the leverage scores of
$A$. By definition, we have

\begin{equation}
p \paren{\bigwedge_{k=1}^{N-1} \hat i_k = a_k} = \hat \ell_{a_1, ..., a_{N-1}}
\label{eq:idx_rv_def}
\end{equation}
Assume, for some $k$, that we have already sampled an index 
from each of $I_1, ..., I_{k-1}$. Concretely, suppose that 
the first $k-1$ random variables take values 
$\hat i_1 = a_1, ..., \hat i_{k-1} = a_{k-1}$, a condition
we abbreviate as $\hat i_{< k} = a_{< k}$. 
To sample from $I_k$, we seek the distribution of 
$\hat i_k$ conditioned on the prior draws. To simplify our 
equations, define $h_{< k}$ as the elementwise product of rows 
already sampled:

$$h_{< k} = \startimes_{u=1}^{k-1} U \br{a_u, :}.$$
Further define $G_{> k}$ as 

$$G_{> k} = G \circledast \startimes_{u=k+1}^{N-1} G_u,$$
the product of gram matrices from matrices we have yet to sample from,
excluding $k$. Then the following theorem adapted to our notation from 
Malik \cite{malik2022} gives the desired conditional probability:

\begin{theorem}[Malik 2022, Adapted]
For any $a_k \in I_k$,
$$p(\hat i_k = a_k\ \vert\ \hat i_{< k} = a_{< k}) = 
C^{-1} \gen{h_{<k}^T h_{<k}, U_k\br{a_k, :}^T U_k \br{a_k, :}, G_{> k}}
$$
where $\gen{\cdot, \cdot, \cdot}$, a generalized inner product, 
denotes sum of all entries in the elementwise product of 
its three arguments and $C = \sum_{i=1}^{\abs{I_k}} \ell_i.$
\label{thm:malik2022}
\end{theorem}
Since we find the proof instructive, we include the derivation of
theorem \ref{thm:malik2022} from equation 
\ref{eq:krp_leverage} in the appendix. The theorem captures 
the difficulty of sampling from the conditional distribution on 
$\hat i_k$: for $k > 1$, the vector $a_{<k}$ is potentially 
unique for each sample drawn. The key to efficient sampling is 
the following observation: for some subset $S \subseteq I_k$, define 

\begin{equation}
G^S_k = \sum_{a_k \in S} U \br{a_k, :}^T U\br{a_k, :}.
\label{eq:partial_gram}
\end{equation}
We call such a matrix a ``partial gram matrix", since it is a sum
of a subset of outer products required to form the complete Gram
matrix $G_k$. Directly from theorem \ref{thm:malik2022}, we have 

\begin{equation} 
C \sum_{a_k \in S} p(\hat i_k = a_k\ \vert\ \hat i_{< k} = a_{< k}) = 
\gen{h_{<k}^T h_{<k}, G^S_k, G_{> k}}
\label{eq:partial_gram_sum}
\end{equation}
The form of equation \ref{eq:partial_gram_sum} mirrors the definition 
of $m(v)$ in proposition \ref{prop:ptsample_correctness}. Further 
note if $G_k^S$ is computed and stored in advance, then
expression on the right-hand side of equation \ref{eq:partial_gram_sum}
is computable in time $O(R^2)$. Armed with this observation, we are
ready to construct an efficient leverage score sampler for the
Khatri-Rao product.

\section{An Efficient KRP Leverage Sampler}
In this section, we present a data structure enabling efficient 
leverage score row sampling of a Khatri-Rao Product matrix. The space overhead of the data structure can be traded off with the runtime of 
drawing samples through configurable parameters. The following
theorem establishes the properties of the proposed structure:

\begin{theorem}[Efficient KRP Leverage Sampling]
Given matrices $U_1, ..., U_N$, there exists a data structure parameterized by integers $F_1, ..., F_N \geq 1$ with the following 
properties:
\begin{enumerate}
    \item The data structure has construction time 
    $$O\paren{\sum_{j=1}^N \abs{I_j} R^2}$$
    and requires storage space
    $$O\paren{\sum_{j=1}^N \paren{\abs{I_j} R^2 / F_j}}.$$
    If matrix $U_j$ changes, the data structure 
    can be updated in time $O(\abs{I_j} R^2)$.

    \item For any index $j$, the data structure produces a single sample from the Khatri-Rao product $U_{\neq j}$ according to the distribution of leverage scores on its rows in time
    $$O\paren{\sum_{k \neq j} \paren{R^2 \log \paren{\abs{I_k} / F_k} + F_k R^2}}$$
    The former term in the summation arises from a sequence of matrix-vector multiplications, and the latter
    arises from a matrix-matrix multiplication.  
\end{enumerate}
\label{thm:main_krp_res}
\end{theorem}
We prove the theorem by exhibit. In the remainder of this section,
we detail the construction, update, and sampling procedures for a data
structure meeting the criteria above. We analyze the time and
space complexity of each procedure before proving the correctness
of our sampler.

\subsection{Construction and Update}
Algorithm \ref{alg:krp_ds_construction} shows how
to build our proposed sampler by initializing a collection $C$ 
of partition trees, one for each matrix $U_j$. For each node 
of each partition tree $C_j$, the procedure computes and stores the partial gram matrix associated with that node's subset. 
For convenience, we denote the partial gram
matrix of node $v$ by $G_j^v$, understood to mean $G_j^{S(v)}$. 
We compute $G_j^v$ for all leaves directly through equation 
\ref{eq:partial_gram}. Proceeding level by level up the tree, we then
compute $G_j^{v}$ for all internal nodes as the sum of
the partial gram matrices of its left and right subchildren. We 
then discard the partial gram matrix stored for each right child,
since our sampling procedure will not require it.

\begin{algorithm}
\caption{KRP Sampler Construction}
\begin{algorithmic}[1]
    \Procedure{ConstructSampler}{$U_1, ..., U_N$}
        \For{$j=1...N$}
            \State Build tree $C_j := T_{\abs{I_j}, F_j}$ with depth $d_j$
            \For{$v \in \lf{C_j}$}
                \State $G_j^{v} := \sum_{i \in S(v)} U_j\br{i, :}^T U_j \br{i, :}$
            \EndFor
            \For{$u=d_j-2...0$}
                \For{$v \in \textrm{level}(C_j, u)$}
                    \State $G_j^{S(v)} := G_j^{L(v)} + G_j^{R(v)}$ 
                    \State $\textrm{memfree} (G_j^{R(v)})$
                \EndFor
            \EndFor
        \EndFor
    \EndProcedure 
\end{algorithmic}
\label{alg:krp_ds_construction}
\end{algorithm}

\textbf{Space Complexity: } We first analyze the space
usage of the trees built in line 3 of \ref{alg:krp_ds_construction}. 
A full binary tree with 
$\ceil{\abs{I_j} / F_j}$ leaves has $2 \ceil{\abs{I_j} / F_j} - 1$
nodes. Only the leaves $v \in \lf{C_j}$ in each tree need to keep 
track of their subset $S(v)$ for sampling 
(see section \ref{sec:sampling}). If each leaf $v$ explicitly stores
the members of $S(v)$, we incur space overhead $O(\abs{I_j})$
over all leaves in $C_j$. Alternatively, if $C_j$ is constructed
so that $S(v)$ is a contiguous interval of $\set{1, ..., \abs{I_j}}$,
only a constant amount of space per node is required to store
the interval endpoints.

The partial gram matrices $G_j^S$ computed in lines 5 and 9 
dominate the space complexity. We compute a partial gram matrix
requiring $O(R^2)$ storage at each node of each tree, but the
construction procedure discards $G_j^v$ whenever $v$ is
a right child (line 10). The space for all partial gram 
matrices (summing over all 
left children and the root in all trees) is 
$$\sum_{j=1}^N \ceil{\abs{I_j} / F_j} R^2.$$

\textbf{Time Complexity: } For tree $C_j$, recall that the subsets
$S(v)$ held by the leaves partition the set $\set{1, ..., \abs{I_j}}$.
Then the loop in lines 4-6 
computes $O(\abs{I_j})$ outer products over
all loop iterations and adds the results to the appropriate
accumulator buffers for partial gram matrices. The cost is 
$O(\abs{I_j} R^2)$. The addition in line 9 requires time $O(R^2)$
per internal node of $C_j$, adding an additional cost 
$O(\abs{I_j} R^2)$. Summed over all $j$, the asymtotic runtime
complexity of constructiion is
$$O \paren{\sum_{j=1}^N \abs{I_j} R^2}$$

\textbf{Update Complexity: } Suppose factor matrix $U_j$ changes after
construction of the data structure.
The update procedure is identical to the construction
procedure. but we run lines 3-12 for only the index $j$ that
requires an update. Following from the time complexity analysis
for construction, the update time for the data structure is 
$O(\abs{I_j} R^2)$.

\subsection{Sampling}
\label{sec:sampling}
The KRPSampleDraw procedure in \ref{alg:krp_ds_sampling} draws a 
single sample from the Khatri-Rao product $U_{\neq j}$ according to
the leverage score distribution on its rows. We assume that the 
trees $C_j$ and the partial gram
matrices $G_j^v$ have been initialized by the construction step
and are accessible in all procedures. Further, we define
two procedures $m, q$ that each accept a vector in $\RR^R$, an
index $1 \leq k \leq N$, and a node $v$ of partition tree
$C_k$. For any vector $h$ and index $k$, the partial evaluations
$m(h, k, \cdot), q(h, k, \cdot)$ define functions on the nodes 
of tree $C_k$ that become arguments to the $\textrm{PTSample}$
procedure. $m$ provides the bias of coin flips at each internal
node of a partition tree, while $q$ directly returns a vector of leverage
scores for an index set held by a leaf.

To sample, we initialize
a ``history vector" $h$ filled with 1's. Each index 
$i_k$ (for $k \neq j$) is drawn according to the 
PTSample procedure applied to tree $C_k$, and the history
vector is updated at each step as the elementwise product
of rows corresponding to each drawn index. The procedure
returns the indices $i_k$ along with $h$, which becomes 
the row drawn from the KRP.

\begin{algorithm}[H]
\caption{KRP $U_{\neq j}$ Sample Draw}
\begin{algorithmic}[1]
    \Procedure{$m$}{$h, k, v$}
        \State \textbf{return} $\gen{h^T h, G_k^v, M_k}$
    \EndProcedure

    \Procedure{$q$}{$h, k, v$}
        \State $X := h^T h \circledast M_k$
        \State $W := U_k\br{S(v), :}$
        \State \textbf{return} $\textrm{diag}(W \cdot X \cdot W^T)$
    \EndProcedure

    \Procedure{KRPDrawSample}{$j$}
        \State Initialize $h := \br{1, ..., 1}^T \in \RR^R$
        \For{$k=1...N$, $k \neq j$}
            \State $\hat i_k := \textrm{PTSample}(C_k, m(h, k, \cdot), q(h, k, \cdot))$ 
            \State $h \timeseq U_k \br{i_k, :}$
        \EndFor
        \State \textbf{return} $(i_1, ..., i_{k-1}, i_{k+1}, ..., i_N), h$
    \EndProcedure 
\end{algorithmic}
\label{alg:krp_ds_sampling}
\end{algorithm}

\textbf{Time Per Sample: } We first analyze the runtime of the 
procedures $m$ and $q$. For any node $v$ in any tree $C_j$, 
procedure $m$ has access to the precomputed matrices $G_k^v$
and $M_k$. As a result, the runtime of line 2 is $O(R^2)$. For procedure
$q$, line 5 requires time $O(R^2)$. The matrix $U_k \br{S(v), :}$ 
in line 6 is composed of rows $U\br{i, :}$ with $i \in S(v)$ 
stacked on top of each other. Its dimensions are $F_j \times R$,
since $\abs{S(v)} \leq F_j$ at each leaf \footnote{If $\abs{S(v)} < v$,
we assign the remaining $F_j - \abs{S(v)}$ rows of $W$ to 0}.
Assembling $W$ costs $O(F_j R^2)$, and returning the
diagonal elements of $W \cdot X \cdot W^T$ in line 7 costs
$O(F_j R^2)$.

We now analyze the runtime of KRPSampleDraw. For any $k \neq j$, the
call to PTSample in line 12 requires time 
$O(\log (\abs{I_k} / F_k) + F_k R^2)$, following from 
equation \ref{eq:ptsample_runtime} with
$\tau_1 = O(R^2)$ and $\tau_2(F_k) = F_k R^2$. Summing over all 
$k \neq j$ gives a per-sample complexity

$$O\paren{\sum_{k \neq j} \paren{\log (\abs{I_k} / F_k)R^2 + F_k R^2}}$$
We have shown that our proposed data structure meets all 
time / space complexity claims of the theorem. It remains to show 
that the probability of sampling a row from $U_{\neq j}$ using
KRPSampleDraw is indeed proportional to the leverage score of that
row.

\subsection{Correctness}
To simplify notation when excluding $U_j$ from the KRP, we 
introduce a reindexing function $t: \set{1,...,N-1} \rightarrow
\set{1, ..., N}$ that operates as follows: $t(k) = k$ for
$1 \leq k < j$, and $t(k) = k+1$ for $k \geq j$. We then set 
$A = U_{\neq j} = U_{t(1)} \odot ... \odot U_{t(N-1)}$, allowing us to match the indexing used in section \ref{sec:krp_leverage_defs}.
To show correctness of our sampler, we prove the following lemma:

\begin{lemma}
    For $a_1, ..., a_{N-1} \in I_{t(1)} \times ... \times 
    I_{t(N-1)}$ indexing a row of the KRP matrix $A$,
    $$p\paren{ \bigwedge_{k=1}^{N-1} i_{t(k)} = a_k} =
    \hat \ell_{a_1, ..., a_{N-1}} 
    $$
    \label{lemma:correctness}
\end{lemma}
Intuitively, we prove this fact by showing that each call
to PTSample samples correctly from the distribution
in \ref{thm:malik2022} conditioned on prior draws. 

\begin{proof}
Recall the definition of $a_{<k}$ and $h_{<k}$ from section
\ref{sec:krp_leverage_defs}. We expand the left-hand side 
of the statement in the lemma using Bayes' rule and line 12 from algorithm \ref{alg:krp_ds_sampling}: 

\begin{aligned_eq} 
    p\paren{ \bigwedge_{k=1}^{N-1} i_{t(k)} = a_k} 
    &= \prod_{k=1}^{N-1} p \paren{ i_{t(k)} = a_k\ \vert\ i_{<t(k)} = a_{<k}} \\
    & = \prod_{k=1}^{N-1} p(\textrm{PTSample}(C_{t(k)}, 
    m(h_{<k}, t(k),  \cdot), q(h_{<k}, t(k), \cdot)) = a_k)
    \label{eq:derivation_part1}
\end{aligned_eq}
We turn our attention to the function $m$:
\begin{aligned_eq}
m(h_{<k}, t(k), v) 
&= \gen{h_{<k}^T h_{<k}, G_{t(k)}^v, M_{t(k)}} & \just{Line 1, alg. \ref{alg:krp_ds_construction}} \\
&= \gen{h_{<k}^T h_{<k}, G_{t(k)}^v, G_{>t(k)}} & \just{Definition of $M_k$} \\
&= C \sum_{a_k \in S(v)} p(\hat i_k = a_k\ \vert\ \hat i_{<k} = a_{<k})
& \just{Equation \ref{eq:partial_gram_sum}}
\label{eq:m_expansion}
\end{aligned_eq}
Likewise for function $q$, consider the $i$'th entry in the vector 
$q(h_{<k}, t(k), v)$ corresponding to element $a_k \in S(v)$. We
have

\begin{aligned_eq}
q(h_{<k}, t(k), v)\br{i}
&= \textrm{diag}(W \cdot X \cdot W^T)\br{i} & \just{Line 7, alg. \ref{alg:krp_ds_construction}} \\
&= U\br{a_k, :} \paren{h_{<k}^T h_{<k} \circledast M_k} U\br{a_k, :}^T \\
&= \gen{h_{<k}^T h_{<k}, U\br{a_k, :}^T U\br{a_k, :}, M_k} \\
&= \gen{h_{<k}^T h_{<k}, U\br{a_k, :}^T U\br{a_k, :}, G_{>k}} \\
&= C p(\hat i_k = a_k \ \vert\ \hat i_{<k} = a_{<k}) 
&\just{Theorem \ref{thm:malik2022}}
\label{eq:q_expansion}
\end{aligned_eq}
Taken together, equations \ref{eq:m_expansion} and \ref{eq:q_expansion}
satisfy the hypothesis of proposition \ref{prop:ptsample_correctness}.
For $1 \leq k \leq N-1$, we then have
\begin{equation} 
p(\textrm{PTSample}(C_{t(k)}, 
    m(h_{<k}, t(k),  \cdot), q(h_{<k}, t(k), \cdot)) = a_k) = 
    p(\hat i_k = a_k\ \vert\ \hat i_{<k} = a_{<k})
    \label{eq:ptsample_conclusion}
\end{equation}
Continuing the derivation in equation \ref{eq:derivation_part1},
we have (applying equation \ref{eq:ptsample_conclusion}, Bayes' Rule again, and equation \ref{eq:idx_rv_def})
\begin{aligned_eq} 
    p\paren{ \bigwedge_{k=1}^{N-1} i_{t(k)} = a_k} 
    & = \prod_{k=1}^{N-1} p(\textrm{PTSample}(C_{t(k)}, 
    m(h_{<k}, t(k),  \cdot), q(h_{<k}, t(k), \cdot)) = a_k) \\
    &= \prod_{k=1}^{N-1} p(\hat i_k = a_k\ \vert\ \hat i_{<k} = a_{<k}) \\
    &= \bigwedge_{k=1}^{N-1} p(\hat i_k = a_k) \\
    &= \hat \ell_{a_1, ..., a_{N-1}},
    \label{eq:derivation_part2}
\end{aligned_eq}
which completes the proof of the lemma.
\end{proof}

\begin{proof}[Proof of Theorem \ref{thm:main_krp_res}]
The data structure exhibited in this section satisfies the time / space complexity claims
from the theorem. Lemma \ref{lemma:correctness} shows that the probability of sampling each 
row from the Khatri-Rao product is proportional to its statistical leverage score. 
\end{proof}

\subsection{Application to ALS CP Decomposition}
Recall that the $j$'th optimization problem in a round of CP-ALS is the linear least-squares problem
$$\min_{\hat U_j} \norm{U_{\neq j} \cdot \diag(\sigma) \cdot \hat U_j^T - \textrm{mat}(\scr T)}_F$$
We can use leverage score sampling and our efficient data structure to reduce the cost of solving this system,
as the following corollary to theorem \ref{thm:main_krp_res} shows.

\begin{corollary}[Complexity of Downsampled ALS, Dense Tensors]
Suppose we replace each linear least-squares problem in CP-ALS by a reduced system that samples a subset of  
$J$ rows from both the design and observation matrices. Taking $\epsilon, \delta \in (0, 1)$, we further require
that with probability $1 - \delta$, the residual of each reduced system is less than 
$(1 + \varepsilon)\textrm{OPT}$, where $\textrm{OPT}$ is the residual of the original problem. If we use
the sampling data structure from theorem \ref{thm:main_krp_res}, the complexity of CP-ALS is
$$
O \paren{\frac{\noiter}{\varepsilon \delta} 
\cdot \sum_{j=1}^N N \paren{R^3 \log (\abs{I_j} / F_j) + F_j R^3 } + \abs{I_j} R^2 }
$$
\label{cor:downsampled_cpals_complexity}
\end{corollary}

\begin{proof}
The design matrix for optimization problem $j$ within a round of ALS has dimensions
$\prod_{k \neq j} \abs{I_k} \times R$, and the observation matrix has dimensions 
$\prod_{k \neq j} \abs{I_k} \times \abs{I_j}$. From theorem \ref{thm:lev_score_lowerbounds}, 
to achieve error threshold $(1 + \varepsilon)$ with probability $1 - \delta$, we set 
$J \gtrsim R / (\varepsilon \delta)$ and sample from the leverage score distribution on the rows of $U_{\neq j}$.
Given a properly initialized sampling data structure, theorem \ref{thm:main_krp_res} gives the complexity
of drawing each sample. To get the cost of drawing $J$ samples, we multiply the per-sample complexity
by $R / (\varepsilon \delta)$. The cost of assembling the corresponding
subset of the observation matrix is $O(J \abs{I_j}) = O(R \abs{I_j} / (\varepsilon \delta))$. Finally, the
cost of solving the downsampled least-squares problem is $O(J R^2) = O(\abs{I_j} R^2 / (\varepsilon \delta))$, which dominates the cost 
of forming the subset of the observation matrix. Adding these terms together and summing over $1 \leq j \leq N$
gives

\begin{aligned_eq}
O &\paren{\frac{1}{\varepsilon \delta} \cdot \sum_{j=1}^N \br{\sum_{k \neq j} 
\paren{R^3 \log (\abs{I_k} / F_k) + F_k R^3 } } + \abs{I_j} R^2 } \\
&= O \paren{\frac{1}{\varepsilon \delta} \cdot \sum_{j=1}^N (N - 1) 
\paren{R^3 \log (\abs{I_j} / F_j) + F_j R^3 } + \abs{I_j} R^2 }
\end{aligned_eq}
Rounding $N-1$ to $N$ and multiplying by the number of iterations gives the desired complexity.
\end{proof}

We now examine the effect of changing the parameters $F_j$. For ease of comparison, we assume here that 
$I = \abs{I_1} = ... = \abs{I_N}$ and $F = F_1 = ... = F_N$. As table \ref{table:f_parameter_effect} shows,
we can vary $F$ to trade off the sampling time and space complexity of our data structure. When $F = 1$, the
partition tree extends to a maximum depth $\log I$ and we achieve the lowest
per-sample complexity. Unfortunately, setting $F=1$ induces space 
overhead $O(N I R^2)$. Since the factor matrices only require space $O(NIR)$, the
asymptotic space usage of the ALS algorithm inflates by a factor $R$, which
may be prohibitive when memory is constrained. We call this algorithm PTS-CP-EM
(extra memory). At the other extreme, setting $F = I$ results in a partition tree
of depth 0 and a high per-iteration complexity $O(N^2 I R^3 / (\varepsilon \delta))$.
This algorithm was previously explored by Malik et al \cite{malik_sdm} under
the name TNS-CP, albeit with a reported complexity 
$O(N^3 I R^3 / (\varepsilon \delta))$. A minor improvement that reuses some 
Hadamard products during sampling results in the lower complexity here. While it
only consumes a small constant amount of extra memory before sampling, TNS-CP
transiently consumes $O(I)$ space to recompute the distribution of leverage
scores on each factor matrix for each sample.

To achieve a reasonable tradeoff, we set $F=R$ to incur $O(NIR)$ space overhead,
linear in the space required to store the factor matrices. We call the resulting
algorithm PTS-CP-LIN (linear), and observe that the per-iteration ALS time increases by
a factor $N^2 R^4$ compared to PTS-CP-EM. this term arises from matrix multiplication in 
procedure $q$ of algorithm \ref{alg:krp_ds_sampling}. By consequence, the enhanced efficiency of matrix-matrix
multiplication (GEMM) reduces the time spent on extra arithmetic in PTS-CP-LIN compared to PTS-CP-EM. 

\begin{table}[]
\centering
\begin{tabular}{|l|l|l|l|}
\hline
\multicolumn{1}{|c|}{\textbf{$F$}} & \multicolumn{1}{c|}{\textbf{Time Per ALS Iteration}} & \multicolumn{1}{c|}{\textbf{Space Overhead}} & \textbf{Algorithm } \\ \hline
$1$       & $O\paren{N \paren{N R^3 \log I + IR^2} / (\epsilon \delta)}$  & $O(NIR^2)$  & PTS-CP-EM  \\
$R$       & $O\paren{N \paren{N R^3 \log (I / R) + NR^4 + IR^2} / (\epsilon \delta)}$  & $O(NIR)$   & PTS-CP-LIN                  \\
$I$       & $O(N^2 I R^3 / (\epsilon \delta))$  & $O(1)$           & TNS-CP                  \\ \hline
\end{tabular}
\vspace{5pt}
\caption{Time-Space tradeoff of sampling data structure. Assumes $I = \abs{I_1} = ... = \abs{I_N}$
and $F = F_1 = ... = F_N$. Space overhead refers only to additional space used before sampling takes place; in
particular, TNS-CP transiently consumes up to $O(I)$ extra memory while sampling.}
\label{table:f_parameter_effect}
\end{table}

\subsection{Parallelization Strategies and Further Optimization}
We now discuss optimizations to the our proposed algorithms that do not change the asymptotic complexity for hypercube-shaped tensors, but reduce downsampling time in practice. We also discuss
approaches to parallel implementation on shared-memory architectures. 

\begin{enumerate}
    \item \textbf{Adjusting $F_j$ for each Mode}: Real-world tensors often exhibit significant imbalances in mode sizes. For example, a 
    $(\textrm{colors} \times \textrm{width} \times \textrm{height})$ tensor representing an RGB image
    has only 3 color channels, but may have width and height on the order of thousands of pixels. When $\abs{I_j}$ is small for some mode $j$, it may be favorable to set $F_j > R$. 
    To see why, recall that the extra time complexity associated with increasing $F_j$ stems mainly from the GEMM call in procedure $q$ of 
    algorithm \ref{alg:krp_ds_sampling}. For small enough mode sizes, a random walk on
    a partition tree is inefficient compared to a GEMM call from an optimized BLAS library. In our experiments, we empirically determine the optimal value of $F_j$ for varying mode sizes $\abs{I_j}$.

    \item \textbf{Asynchronous Thread Parallelism}: When run on multiple threads, the KRPSampleDraw procedure in algorithm \ref{alg:krp_ds_sampling} is ``proudly parallel". 
    To draw $J$ rows from the KRP design matrix, we can initialize a thread team and assign each thread a roughly equal count of samples to draw. These threads make independent calls to
    KRPSampleDraw asynchronously, efficiently parallelizing the process of selecting rows. The MTTKRP of each downsampled least-squares system can be computed using a thread-parallel 
    GEMM call when the tensor $\scr T$ is dense. When $\scr T$ is sparse, we compute the MTTKRP by looping over all the nonzeros of $\scr T$ using a computation pattern similar to
    parallel sparse-dense matrix multiplication (SpMM). All remaining operations, such as multiplication of the MTTKRP output by $G^+$ or rescaling the columns of the output factor, are
    also parallelized through either a threaded BLAS call or an OpenMP-style parallel \verb|for| loop.

    \item \textbf{Synchronous Batch Parallelism} As an alternative to asynchronously calling PTSample, suppose for the moment that all leaves have the same depth in each partition tree. Then for every sample, 
    PTSample makes an sequence of calls to $m$ and conditional branches to update the current node. The length of this sequence is the depth of the tree, and it is followed by a single call to the function $q$. As
    a result, the PTSample procedure can be implemented through a fixed-length sequence of calls to functions $m$ and $q$ that take a batch of inputs instead of a single input. We can implement 
    these batched versions conveniently using available batched BLAS routines or simple custom kernels. In contrast to the asynchronous 
    strategy, all cores march in lock-step down the levels of the partition tree, each of them tracking the branching path of 
    a single sample. Such an approach is preferable when implementing our sampler on SIMT machines, such as GPUs. 

\end{enumerate}

\section{Comparison to Established Methods}

\begin{table}[htb]
	\centering
	\begin{tabular}{ll}  
		\hline
		Method    											& Complexity 												\\
		\hline
		CP-ALS \cite{kolda2009TensorDecompositions}		& $\noiter \cdot N (N+I) I^{N-1} R$ 						\\
		SPALS \cite{cheng2016SPALSFast} 					& $I^N + \noiter \cdot N (N+I) R^{N+1} / \tilde{\varepsilon}^2$ 			\\	
		CP-ARLS-LEV	\cite{larsen2022PracticalLeverageBased}& $\noiter \cdot N ( R + I ) R^N / (\varepsilon \delta)$	\\
		TNS-CP (\cite{malik2022arbitrary})	   			& $\noiter \cdot N^3 I R^3 / (\varepsilon \delta)$ 			\\ 
		Ma and Solomonik 2022 (\cite{maCostEfficient})	   			& $\noiter \cdot N^2(N^{1.5} R^{3.5} / \varepsilon^3 + I R^2) / \varepsilon^2$ 			\\ 
		\textbf{PTS-CP-LIN (ours)}	   			& $\noiter \cdot N \cdot (N R^3 \log (I/R) + NR^4 + IR^2) / (\varepsilon \delta)$ 			\\ 
		\textbf{PTS-CP-EM (ours)}	   			& $\noiter \cdot N \cdot (N R^3 \log I + IR^2) / (\varepsilon \delta)$ 			\\
		\hline
	\end{tabular}
        \vspace{3pt}
	\caption{Table modified from Malik et al. Factors involving $\log R$ and 
        $\log (1 / \delta)$ have been dropped.} 
    \label{tab:cp-complexity-comparison}
\end{table}


