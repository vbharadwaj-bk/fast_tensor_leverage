{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: 1: source: not found\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compiling C++ extensions with ['--std=c++2a', '-fopenmp', '-Ofast', '-march=native', '-I/opt/OpenBLAS/include/']\n",
      "Linking C++ extensions with ['-fopenmp', '-Ofast', '-L/opt/OpenBLAS/lib/', '-lopenblas']\n"
     ]
    },
    {
     "ename": "ImportError",
     "evalue": "/media/hrluo/WORK1/fast_tensor_leverage/cpp_ext/als_module.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3tbb6detail2r15spawnERNS0_2d14taskERNS2_18task_group_contextE",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [1], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m os\u001b[39m.\u001b[39msystem(\u001b[39m\"\u001b[39m\u001b[39msource env.sh\u001b[39m\u001b[39m\"\u001b[39m)  \n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mcppimport\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mimport_hook\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mcpp_ext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mals_module\u001b[39;00m \u001b[39mimport\u001b[39;00m Sampler\n\u001b[1;32m      7\u001b[0m \u001b[39m# Take J=10,000 samples from the KRP of N=4 matrices,\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39m# each with dimensions I x R, where I = 1000 and R = 8.\u001b[39;00m\n\u001b[1;32m      9\u001b[0m I, N, J, R \u001b[39m=\u001b[39m \u001b[39m1000\u001b[39m, \u001b[39m4\u001b[39m, \u001b[39m10000\u001b[39m, \u001b[39m8\u001b[39m\n",
      "\u001b[0;31mImportError\u001b[0m: /media/hrluo/WORK1/fast_tensor_leverage/cpp_ext/als_module.cpython-310-x86_64-linux-gnu.so: undefined symbol: _ZN3tbb6detail2r15spawnERNS0_2d14taskERNS2_18task_group_contextE"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "os.system(\"./env.sh\")  \n",
    "import cppimport.import_hook\n",
    "from cpp_ext.als_module import Sampler\n",
    "\n",
    "# Take J=10,000 samples from the KRP of N=4 matrices,\n",
    "# each with dimensions I x R, where I = 1000 and R = 8.\n",
    "I, N, J, R = 1000, 4, 10000, 8\n",
    "matrices = [np.random.normal(size=(I, R)) for i in range(N)]\n",
    "\n",
    "sampler = Sampler(matrices, J, R, \"efficient\")\n",
    "samples = np.zeros((N, J), dtype=np.uint64)\n",
    "sampler.KRPDrawSamples(N+1, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#def f(X, noise=noise):\n",
    "#    return -np.sin(3*X) - X**2 + 0.7*X + noise * np.random.randn(*X.shape)\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "# insert at 1, 0 is the script path (or '' in REPL)\n",
    "sys.path.insert(1, '../Code/')\n",
    "from cpd import *\n",
    "MYSEED = 40\n",
    "np.random.seed(MYSEED)\n",
    "total_budget = 50 # Maximum number of iterations\n",
    "tol = 0#1e-4#1e-32 # Tolerance in tensor decomposition\n",
    "\n",
    "if True:\n",
    "    sz = [200, 200, 200] # Size of tensor\n",
    "    N = len(sz) # Number of modes\n",
    "    R = 130 # Rank\n",
    "    noise = 1e-1#0.#1e-24 # Amount of noise to add to generated tensor\n",
    "    # # Sketch size for sampled CP decomposition\n",
    "    factors_true = [randn(size=(sz[k], R)) for k in range(N)]\n",
    "    X = factors_to_tensor(factors_true) + noise*randn(size=sz)\n",
    "    #Tunable parameters\n",
    "    #sketch_size: 3\n",
    "    #sketch_switch: 3-1 = 2 \n",
    "    # Generate random factor matrices and compute corresponding tensor plus noise\n",
    "else:\n",
    "    factors_true = [randn(size=(sz[k], R)) for k in range(N)]\n",
    "    #X = factors_to_tensor(factors_true) + noise*randn(size=sz)\n",
    "    outputs_true = np.load(\"../220415/generated_data/generate_tile_plot_seed_1.npy\")\n",
    "\n",
    "    output_true_pre = [np.transpose(outputs_true[0, :, :, s]) for s in [0,1,2] ]\n",
    "    sz = [s.shape[0] for s in output_true_pre ]\n",
    "\n",
    "    X = factors_to_tensor(output_true_pre) + noise*randn(size=sz)\n",
    "    R = output_true_pre[0].shape[1]\n",
    "\n",
    "# Factorize using deterministic method and create corresponding tensor\n",
    "factors = cpd(X, R, tol=tol, maxiters=total_budget)\n",
    "Y = factors_to_tensor(factors)\n",
    "rel_er = norm(X-Y)/norm(X)\n",
    "print('Exact CPD, rel_err',rel_er)\n",
    "factors_sampled, out_sampled, timing_sampled = cpd_sampled(X, R, tol=tol, maxiters=100, sketch_size=4*R, seed=42, output_stats=True, timeit=1)\n",
    "print('Sampled CPD, rel_err',out_sampled)\n",
    "plt.plot(np.array(out_sampled).squeeze())\n",
    "plt.plot(np.array(out_sampled).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take the best-so-far error to make sure that the error sequence is monotonic decreasing\n",
    "def bestError(out_seq):\n",
    "    res = []\n",
    "    for i in range(len(out_seq)):\n",
    "        if i == 0: \n",
    "            res.append(out_seq[0])\n",
    "            continue\n",
    "        res.append(np.amin(out_seq[0:i]))\n",
    "    return res\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.plot( range(len(out_sampled)),bestError(out_sampled) )\n",
    "def intervalToAccumulated(timingRecord):\n",
    "    accumulatedTimingRecord = []\n",
    "    for i in range(len(timingRecord)):\n",
    "        accumulatedTimingRecord.append(sum(timingRecord[0:i]))\n",
    "    return accumulatedTimingRecord\n",
    "    #print(len(timingRecord),len(accumulatedTimingRecord))\n",
    "plt.figure()\n",
    "plt.plot( range(len(out_sampled)),intervalToAccumulated(timing_sampled))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import gpflow\n",
    "gpflow.config.set_default_float(np.float64)\n",
    "gpflow.config.set_default_jitter(1e-3)\n",
    "import tensorflow as tf\n",
    "from tensorflow_probability import bijectors as tfb\n",
    "def bounded_parameter(low, high, default):\n",
    "    \"\"\"Make lengthscale tfp Parameter with optimization bounds.\"\"\"\n",
    "    #affine = tfb.AffineScalar(shift=tf.cast(low, tf.float64),\n",
    "    #                          scale=tf.cast(high-low, tf.float64))\n",
    "    affine_scale = tfb.Scale(scale=tf.cast(high-low, tf.float64))\n",
    "    affine_shift = tfb.Shift(shift=tf.cast(low, tf.float64))\n",
    "    sigmoid = tfb.Sigmoid()\n",
    "    logistic = tfb.Chain([affine_shift, affine_scale, sigmoid])\n",
    "    parameter = gpflow.Parameter(default, transform=logistic, dtype=tf.float64)\n",
    "    return parameter\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import ConstantKernel, Matern, WhiteKernel, RBF\n",
    "#from bayesian_optimization_util import plot_approximation, plot_acquisition\n",
    "\n",
    "# Gaussian process with Mat√©rn kernel as surrogate model\n",
    "m52 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "gpr = GaussianProcessRegressor(kernel=m52, alpha=noise**2)\n",
    "\n",
    "# Initialize samples\n",
    "#X_sample = X_init\n",
    "#Y_sample = Y_init\n",
    "\n",
    "# Update Gaussian process with existing samples\n",
    "#gpr.fit(X_sample, Y_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Step 1: Start from a randomly chosen small sketch rate, say 10, fix this sketch rate and run a few steps,\n",
    "iter_size_history = [3]*17\n",
    "sketch_size_history = [R,2*R,3*R,4*R,5*R,6*R,7*R,8*R,9*R,10*R]\n",
    "#assert len(iter_size_history)==len(sketch_size_history)\n",
    "factors_history = [None]\n",
    "sketch_pointer = 0\n",
    "total_iter = 0\n",
    "#For plots\n",
    "bestErrorRecord = []\n",
    "sketchRateRecord = []\n",
    "timingRecord = []\n",
    "continue_counter = 0\n",
    "#total_budget = 50\n",
    "VERBOSE = False\n",
    "#This is a sketch size decomposition, the sum(sketch_size_history) would be the maximal \n",
    "while total_iter<total_budget:\n",
    "    sketch_size = sketch_size_history[sketch_pointer]\n",
    "    current_factor = factors_history[sketch_pointer]\n",
    "    iter_size = 3 +int(sketch_size/500)#Basic iteration size\n",
    "    #iter_size = iter_size_history[sketch_pointer]\n",
    "    #Perhaps work with Osman for better error handling:\n",
    "    try:#Should we really change the random seed?\n",
    "        print('Now we use cpd_sampled with maxiters=',iter_size+continue_counter)\n",
    "        factors_rand, out_rand, timing_rand = cpd_sampled(X, R, tol=tol, maxiters=iter_size+continue_counter, seed=MYSEED+1,\\\n",
    "                                                 sketch_size=sketch_size, output_stats=True, factors=current_factor, timeit=True, verbose = VERBOSE)\n",
    "    except:\n",
    "        print('tolerance reached error')\n",
    "        break\n",
    "    factors_history.append(factors_rand)\n",
    "    print('factors_history length=',len(factors_history))\n",
    "    print('best error so far:',bestErrorRecord)\n",
    "#Step 2: Run a few using regular GP-EI, and then maintain a relative error model as above. And the relative error model would get more accurate. \n",
    "# Currently, there is no EI since the sketch_size is the only parameter to be tuned. Therefore, we are using the existing step-size to model the performance of CPD relative error.\n",
    "    # Gaussian process with Mat√©rn kernel as surrogate model\n",
    "    # Fit a Gaussian process with existing samples\n",
    "    #X_sample = np.array(range(iter_size+continue_counter))\n",
    "    X_sample = np.array(range(len(out_rand)))\n",
    "    X_sample = X_sample.astype(np.float64).reshape(-1,1)\n",
    "    Y_sample = bestError(out_rand) + np.random.normal(size=(1,len(out_rand) ))*1e-10\n",
    "    Y_sample = (np.array(Y_sample).astype(np.float64)).reshape(-1,1)\n",
    "    try:\n",
    "        assert Y_sample.shape[0]==X_sample.shape[0]\n",
    "        #print(X_sample,Y_sample)\n",
    "    except:\n",
    "        print('unequal error, X_sample.shape=',X_sample.shape)\n",
    "        print('unequal error, Y_sample.shape=',Y_sample.shape)\n",
    "        break\n",
    "    for j in range(len(timing_rand)):\n",
    "        timingRecord.append(timing_rand[j])\n",
    "        bestErrorRecord.append(bestError(out_rand)[j])\n",
    "        sketchRateRecord.append(sketch_size)\n",
    "        if VERBOSE: print('>>>>>>',len(bestErrorRecord),len(sketchRateRecord))\n",
    "    #\n",
    "    if False:\n",
    "        m52 = ConstantKernel(1.0) * Matern(length_scale=1.0, nu=2.5)\n",
    "        gpr = GaussianProcessRegressor(kernel=m52, alpha=noise**2)\n",
    "\n",
    "        gpr.fit(X_sample, Y_sample)\n",
    "\n",
    "        rel_error_gp = gpr.predict(np.array(range(total_budget)).reshape(-1,1))\n",
    "    else:\n",
    "        X_dim = 1\n",
    "        kernel = gpflow.kernels.Matern32(active_dims=range(X_dim),lengthscales=[1.]*X_dim) #+ gpflow.kernels.White()\n",
    "        # frequentist fit\n",
    "        global_noise_variance_lower = 1e-5\n",
    "        global_noise_variance_upper = 1e-3\n",
    "        global_noise_variance_init = 0.5*(global_noise_variance_lower+global_noise_variance_upper)\n",
    "\n",
    "        model = gpflow.models.GPR(data=(X_sample, Y_sample),\\\n",
    "                        kernel=kernel,\\\n",
    "                        mean_function=None,\\\n",
    "                        #mean_function=mean_function,\\\n",
    "                        noise_variance=global_noise_variance_init)\n",
    "        # We optimize this using Scipy\n",
    "        model.likelihood.variance = bounded_parameter(global_noise_variance_lower, global_noise_variance_upper, global_noise_variance_init)\n",
    "        if VERBOSE: gpflow.utilities.print_summary(model)\n",
    "\n",
    "        opt = gpflow.optimizers.Scipy()\n",
    "        n_maxiter = 1000\n",
    "        option_opt = dict(disp=False, maxiter=n_maxiter,seed=MYSEED)\n",
    "        try:\n",
    "            opt_logs = opt.minimize(model.training_loss, model.trainable_variables, method='L-BFGS-B', options=option_opt)\n",
    "            # Print output: optimization monitoring, estimated kernel hyperparameters, coregionalization matrix\n",
    "            if VERBOSE:  print(opt_logs)\n",
    "            if VERBOSE:  gpflow.utilities.print_summary(model)\n",
    "            likelihood = model.log_marginal_likelihood()\n",
    "            if VERBOSE:  tf.print(f\"Optimizer: L-BFGS-B loglik_marg: {likelihood: .04f}\")      \n",
    "            gp_mean, gp_cov = model.predict_f(np.array(range(total_budget)).astype(float).reshape(-1,1),full_cov=False)\n",
    "\n",
    "            rel_error_gp = gp_mean.numpy()#.reshape(1,-1)\n",
    "            rel_error_gp_cov = gp_cov.numpy()\n",
    "            IMPROVEMENT_GP = True\n",
    "    \n",
    "        except:\n",
    "            rel_error_gp = np.array(range(total_budget)).astype(float).reshape(-1,1)*0.\n",
    "            #Non-invertible covariance matrix in GP, caused by no-improvement.\n",
    "            IMPROVEMENT_GP = False\n",
    "\n",
    "    rel_error_gp_grad = np.gradient(np.array(rel_error_gp).squeeze())\n",
    "    if VERBOSE: \n",
    "        plt.scatter(np.array(range(iter_size+continue_counter)).reshape(-1,1),np.array(out_rand).reshape(-1,1),label='relative error(raw)',color='blue' )\n",
    "        plt.scatter(np.array(range(iter_size+continue_counter)).reshape(-1,1),np.array(bestError(out_rand)).reshape(-1,1),label='relative error(best)',color='red' )\n",
    "        plt.plot(np.array(range(total_budget)).reshape(-1,1),rel_error_gp.reshape(-1,1),label='GP sketch rate='+str(sketch_size),color='red')\n",
    "        plt.plot(rel_error_gp_grad,label='GP slope',color='green')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "#Step 3: If the relative error model says there is not much improvement ahead, we switch to another (larger) sketch rate, using a steplength, say +10.\n",
    "#(Once the model is deemed to be accurate enough, the trade-off would be goodness of fit and the number of samples before switch) This step is tentative, need to discuss with Osman/Jim/Sherry/Yang\n",
    "    #Find the first index where the gradient of the GP prediction falls below.\n",
    "    grad_threshold = 0.01\n",
    "    seq_threshold = np.argwhere( np.abs(rel_error_gp_grad[(iter_size+continue_counter):-1])> grad_threshold ) \n",
    "    if len(seq_threshold)>0:\n",
    "        iter_goal = max(seq_threshold)[0]\n",
    "    else:\n",
    "        iter_goal = 0\n",
    "    if VERBOSE: print(seq_threshold,iter_goal)\n",
    "    #This expression above is the last iteration count where the norm of gradient (or simpler, the absolute value of gradient) is at least as large as grad_threshold (so that we can deem enough improvement until this step)\n",
    "    if iter_size+continue_counter < iter_goal and IMPROVEMENT_GP:\n",
    "        factors_history.pop()\n",
    "        for j in range(len(timing_rand)):\n",
    "            #itm = timing_rand[j]\n",
    "            timingRecord.pop()#append(itm)\n",
    "            bestErrorRecord.pop()#.append(out_rand[j])\n",
    "            sketchRateRecord.pop()#.append(sketch_size)\n",
    "        print('Yes> It is worth doing a few more steps at sketch rate=',sketch_size,'until ',iter_goal,' total iteration at this rate. continue_counter=',continue_counter)\n",
    "        continue_counter = continue_counter + 1\n",
    "    else:\n",
    "        total_iter = total_iter + (iter_size + continue_counter)\n",
    "        if sketch_pointer<len(sketch_size_history)-1:\n",
    "            sketch_pointer = sketch_pointer + 1\n",
    "            continue_counter = 0\n",
    "        else:\n",
    "            print('No usable sketch_rate any more from the list.')\n",
    "            #break\n",
    "            #use the rest budget at highest sketch_rate\n",
    "            continue_counter = total_budget - total_iter - iter_size\n",
    "        \n",
    "        print('No> It is not worth doing any more steps, next sketch rate=',sketch_size_history[sketch_pointer])\n",
    "#Step 4: Repeat steps 2 and 3 until we are running out of sampling budget (or to look at the plateau of previous GP process before switching, so that we could get an accurate solution as fast as possible). \n",
    "# What is an appropriate termination step? The following is a simple termination step. \n",
    "    print('Total steps so far:',total_iter)\n",
    "    if total_iter>total_budget: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.gradient(np.array(out_sampled).squeeze()))\n",
    "plt.plot(np.gradient(np.array(bestError(out_sampled)).squeeze()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for i in np.unique(sketchRateRecord):\n",
    "#    print(i)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "plt.scatter(range(len(bestErrorRecord)),bestErrorRecord,c=sketchRateRecord)\n",
    "plt.xlabel('Total iteration')\n",
    "plt.ylabel('Relative error')\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(intervalToAccumulated(timingRecord)[0:len(bestErrorRecord)],bestErrorRecord,c=sketchRateRecord)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Accumulated time(s)')\n",
    "plt.ylabel('Relative error')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.asarray(sketchRateRecord)[np.where(np.asarray(bestErrorRecord) >0.8)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plotData_out_sampled = []\n",
    "plotData_timing_sampled = []\n",
    "plotData_out_acc = []\n",
    "plotData_timing_acc =[]\n",
    "print(MYSEED)\n",
    "for sketch_size_fixed in sketch_size_history:\n",
    "    print('sketch_size_fixed=',sketch_size_fixed)\n",
    "    factors_sampled, out_sampled, timing_sampled = cpd_sampled(X, R, tol=tol, maxiters=total_budget, sketch_size=sketch_size_fixed, seed=MYSEED+1, output_stats=True, timeit=True, verbose=False)\n",
    "    #factors_sampled, out_sampled, timing_sampled = cpd_sampled(X, R, tol=tol, maxiters=total_budget, sketch_size=sketch_size_fixed, seed=43, output_stats=True, timeit=True)\n",
    "    plotData_out_sampled.append(out_sampled)\n",
    "    plotData_timing_sampled.append(timing_sampled)\n",
    "    plotData_out_acc.append(bestError(out_sampled))\n",
    "    plotData_timing_acc.append(intervalToAccumulated(timing_sampled))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(sketch_size_fixed,tol)\n",
    "#cpd_sampled(X, R, tol=tol, maxiters=50, sketch_size=100, seed=MYSEED, output_stats=True, timeit=True, verbose=True)\n",
    "#plt.scatter( range(50),plotData_timing_acc[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print('>>>>>>',len(bestErrorRecord),len(sketchRateRecord))\n",
    "import json\n",
    "with open('CPD_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'_plotData_out_sampled.json', \"w\") as fp:\n",
    "    json.dump(plotData_out_sampled, fp)\n",
    "with open('CPD_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'_plotData_timing_sampled.json', \"w\") as fp:\n",
    "    json.dump(plotData_timing_sampled, fp)\n",
    "with open('CPD_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'_plotData_out_acc.json', \"w\") as fp:\n",
    "    json.dump(plotData_out_acc, fp)\n",
    "with open('CPD_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'_plotData_timing_acc.json', \"w\") as fp:\n",
    "    json.dump(plotData_timing_acc, fp)\n",
    "np.savetxt('Adapt_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'_bestError.csv',np.asarray(bestErrorRecord).squeeze())\n",
    "np.savetxt('Adapt_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'_bestTime.csv',np.asarray(intervalToAccumulated(timingRecord)).squeeze())\n",
    "np.savetxt('Adapt_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'_sketchRateRecord.csv',np.asarray(sketchRateRecord).squeeze())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Let's do an envelop fitting using GP. It seems that the medium sketch rate behaves the best in terms of relative error?\n",
    "X_total_sample = np.array( range(len(bestErrorRecord)) ).reshape(-1,1)\n",
    "Y_total_sample = np.array( bestErrorRecord ).reshape(-1,1)\n",
    "m52 = RBF()\n",
    "gpr = GaussianProcessRegressor(kernel=m52, alpha=0.1**2)\n",
    "\n",
    "mycmap = plt.get_cmap('jet', len(sketch_size_history))\n",
    "\n",
    "gpr.fit(X_total_sample, Y_total_sample)\n",
    "plt.figure(figsize=(10,8))\n",
    "import matplotlib\n",
    "from colour import Color\n",
    "red = Color(\"red\")\n",
    "mycmap = list(red.range_to(Color(\"green\"),10))\n",
    "mymatplotcmap = matplotlib.colors.ListedColormap([s.hex_l for s in mycmap])\n",
    "\n",
    "#plt.plot( gpr.predict(np.array(range(50)).reshape(-1,1)),label='GP envelope' )\n",
    "counter = 0\n",
    "for sketch_size_fixed in sketch_size_history:\n",
    "    #print('sketch_size_fixed=',sketch_size_fixed)\n",
    "    used_iter = len(plotData_out_acc[counter])\n",
    "    plt.plot( plotData_timing_acc[counter],plotData_out_acc[counter], label= 'CPD, sketch_size='+str(sketch_size_fixed)+'\\nrel error='+str(np.round(plotData_out_acc[counter][-1],6))+'\\nn_iter='+str(used_iter),color=mycmap[counter].hex_l,linewidth=3 )\n",
    "    plt.scatter( plotData_timing_acc[counter],plotData_out_acc[counter], color=mycmap[counter].hex_l)\n",
    "    #plt.plot( plotData_timing_acc[counter],plotData_out_sampled[counter], label= 'CPD, sketch_size='+str(sketch_size_fixed)+'\\nrel error='+str(np.round(plotData_out_acc[counter][-1],6))+'\\nn_iter='+str(used_iter),color=mycmap[counter].hex_l,linewidth=3 )\n",
    "    #plt.scatter( plotData_timing_acc[counter],plotData_out_sampled[counter], color=mycmap[counter].hex_l)\n",
    "    #plt.text(x=intervalToAccumulated(timing_sampled)[-1]+0.06*counter, y=bestError(out_sampled)[-1], s='sketch_size:'+str(sketch_size_fixed)+'\\nrel err:'+str(np.round(bestError(out_sampled)[-1],6)),rotation=90, size=10)\n",
    "    counter = counter + 1\n",
    "\n",
    "plt.plot(intervalToAccumulated(timingRecord),bestErrorRecord,color='gray',linewidth=5,zorder=99)\n",
    "plt.plot(intervalToAccumulated(timingRecord),bestErrorRecord,color='white',linewidth=3,zorder=99)\n",
    "plt.scatter(intervalToAccumulated(timingRecord),bestErrorRecord,c=sketchRateRecord,cmap=mymatplotcmap,zorder=99)\n",
    "used_iter = len(bestErrorRecord)\n",
    "plt.text(x=intervalToAccumulated(timingRecord)[-1], y=bestErrorRecord[-1], s='Adaptive, rel err='+str(np.round(bestErrorRecord[-1],6))+'n_iter='+str(used_iter),rotation=90, size=15)\n",
    "plt.colorbar(extend=\"max\",location=\"bottom\",fraction=0.15)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "plt.ylim(0.,2.)\n",
    "#plt.xlabel('Total iteration')\n",
    "plt.xlabel('Accumulated time (s)')\n",
    "plt.ylabel('Relative error (logarithm scale)')\n",
    "plt.title('CPD for a 3-way tensor of size '+str(sz)+',R='+str(R))\n",
    "plt.savefig('CPD_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'.png', dpi=600,bbox_inches = \"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_passage(time_seq_acc,error_seq_acc,threshold,return_time=True,cutoff=np.inf):#If false, return first passage index\n",
    "    if return_time:\n",
    "        whereseq = np.where(np.asarray(error_seq_acc)<threshold)\n",
    "        if len(whereseq[0])>0:\n",
    "            return time_seq_acc[whereseq[0][0]]\n",
    "        else:\n",
    "            return cutoff\n",
    "    else:\n",
    "        whereseq = np.where(np.asarray(error_seq_acc)<threshold)\n",
    "        if len(whereseq[0])>0:\n",
    "            return whereseq[0][0]\n",
    "        else:\n",
    "            return cutoff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold_seq = 10**(-1.*np.linspace(0,6,100))\n",
    "plt.figure(figsize=(10,2))\n",
    "thres_time_seq = []\n",
    "MAXTIME = 45\n",
    "for thres in threshold_seq:\n",
    "    thres_time = first_passage(intervalToAccumulated(timingRecord),bestErrorRecord,thres,False,MAXTIME)  \n",
    "    #plt.vlines(x=thres, ymin=0, ymax=thres_time,linewidth=50)\n",
    "    thres_time_seq.append(thres_time)\n",
    "plt.plot(threshold_seq, thres_time_seq, linewidth=5,color='gray')\n",
    "plt.plot(threshold_seq, thres_time_seq, linewidth=3,color='white',zorder=99,label='Adaptive')\n",
    "counter = 0\n",
    "for sketch_size_fixed in sketch_size_history:\n",
    "    print('sketch_size=',sketch_size_history[counter])\n",
    "    thres_counter = 0\n",
    "    thres_time_seq = []\n",
    "    for thres in threshold_seq:\n",
    "        thres_time =  first_passage(plotData_timing_acc[counter],plotData_out_acc[counter],thres,False,MAXTIME)  \n",
    "        #plt.vlines(x=thres, ymin=0, ymax=thres_time+(counter*(threshold_seq[thres_counter + 1]-thres)/len(threshold_seq)),linewidth=50,color=mycmap[counter].hex_l)\n",
    "        thres_counter = thres_counter + 1\n",
    "        thres_time_seq.append(thres_time)\n",
    "    plt.plot(threshold_seq, thres_time_seq,linewidth=5,color=mycmap[counter].hex_l,label= 'CPD, sketch_size='+str(sketch_size_fixed))\n",
    "    counter = counter + 1\n",
    "    #print('sketch_size_fixed=',sketch_size_fixed)\n",
    "    #factors_sampled, out_sampled, timing_sampled = cpd_sampled(X, R, tol=tol, maxiters=50, sketch_size=sketch_size_fixed, seed=sketch_size_fixed, output_stats=True, timeit=True, verbose=False)\n",
    "    #plt.plot( plotData_timing_acc[counter],plotData_out_acc[counter], label= 'CPD, sketch_size='+str(sketch_size_fixed)+'\\nrel error='+str(np.round(plotData_out_acc[counter][-1],6)),color=mycmap[counter].hex_l,linewidth=3 )\n",
    "    #plt.scatter( plotData_timing_acc[counter],plotData_out_acc[counter], color=mycmap[counter].hex_l)\n",
    "    #plt.text(x=intervalToAccumulated(timing_sampled)[-1]+0.06*counter, y=bestError(out_sampled)[-1], s='sketch_size:'+str(sketch_size_fixed)+'\\nrel err:'+str(np.round(bestError(out_sampled)[-1],6)),rotation=90, size=10)\n",
    "    #counter = counter + 1\n",
    "plt.xlim(min(threshold_seq),1)\n",
    "plt.xscale('log')\n",
    "plt.xlabel('tolerance')\n",
    "plt.ylabel('# iterations')\n",
    "#plt.legend()\n",
    "plt.savefig('CPD_'+str(MYSEED)+'_'+str(sz)+'_R_'+str(R)+'_'+str(tol)+'_'+str(total_budget)+'_ITER.png', dpi=600,bbox_inches = \"tight\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
